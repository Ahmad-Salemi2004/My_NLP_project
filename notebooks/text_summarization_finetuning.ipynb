!pip install datasets transformers torch accelerate evaluate rouge_score nltk --quiet

import torch
import numpy as np
import pandas as pd
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, TrainingArguments, Trainer
from transformers import pipeline
import evaluate
import nltk
from nltk.tokenize import sent_tokenize
import gc

# Download NLTK data
nltk.download('punkt')

# Check for GPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load the DialogSum dataset
print("Loading DialogSum dataset...")
ds = load_dataset("knkarthick/dialogsum")

print(f"\nDataset loaded successfully!")
print(f"Training samples: {len(ds['train'])}")
print(f"Validation samples: {len(ds['validation'])}")
print(f"Test samples: {len(ds['test'])}")

# Show an example
print("\nExample from training set:")
print("Dialogue:")
print(ds['train'][0]['dialogue'])
print("\nSummary:")
print(ds['train'][0]['summary'])

# Load BART model and tokenizer
print("\nLoading BART-large-cnn model...")
model_name = "facebook/bart-large-cnn"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
model = model.to(device)

print(f"Model and tokenizer loaded successfully!")

# Preprocessing function for the dataset
def preprocess_function(batch):
    source = batch['dialogue']
    target = batch['summary']
    
    source_ids = tokenizer(source, truncation=True, padding='max_length', max_length=128)
    target_ids = tokenizer(target, truncation=True, padding='max_length', max_length=128)
    
    labels = target_ids['input_ids']
    labels = [[(label if label != tokenizer.pad_token_id else -100) for label in labels_test] for labels_test in labels]
    
    return {
        'input_ids': source_ids['input_ids'],
        'attention_mask': source_ids['attention_mask'],
        'labels': labels
    }

# Apply preprocessing to the dataset
print("\nPreprocessing dataset...")
df_source = ds.map(preprocess_function, batched=True)
print("Preprocessing completed!")

# Set up training arguments
training_args = TrainingArguments(
    output_dir="./results",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=100,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    greater_is_better=False,
    remove_unused_columns=True
)

# Initialize the trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=df_source['train'],
    eval_dataset=df_source['validation']
)

# Train the model
print("\nStarting model training...")
train_result = trainer.train()
print("\nTraining completed!")
print(f"Final training loss: {train_result.training_loss:.4f}")

# Evaluate on test set
print("\nEvaluating on test set...")
eval_results = trainer.evaluate(eval_dataset=df_source['test'])
print("\nTest set evaluation results:")
for key, value in eval_results.items():
    print(f"{key}: {value:.4f}")

# Save the trained model
model_path = "./fine_tuned_bart"
model.save_pretrained(model_path)
tokenizer.save_pretrained(model_path)
print(f"\nModel saved to: {model_path}")

# Test the model with a sample
print("\nTesting the trained model...")

# Clean up memory before loading pipeline
del model
del trainer
gc.collect()
if torch.cuda.is_available():
    torch.cuda.empty_cache()

# Load the fine-tuned model using pipeline
summarizer = pipeline(
    "summarization",
    model=model_path,
    tokenizer=model_path,
    device=0 if torch.cuda.is_available() else -1
)

# Function to generate summaries
def summarize_text(text):
    output = summarizer(
        text,
        max_length=150,
        min_length=40,
        length_penalty=2.0,
        num_beams=4,
        early_stopping=True
    )
    return output[0]['summary_text']

# Test with a sample from the test set
sample_idx = 0
test_dialogue = ds['test'][sample_idx]['dialogue']
test_summary = ds['test'][sample_idx]['summary']

print("\nTest Sample from Dataset:")
print("=" * 50)
print("Original Dialogue:")
print(test_dialogue)
print("\nOriginal Summary:")
print(test_summary)
print("\nGenerated Summary:")
generated_summary = summarize_text(test_dialogue)
print(generated_summary)

# Test with custom text
custom_text = """
The ultimate productivity hack is saying no.

Not doing something will always be faster than doing it. This statement reminds me of the old computer programming saying, "Remember that there is no code faster than no code."

The same philosophy applies in other areas of life. For example, there is no meeting that goes faster than not having a meeting at all.

This is not to say you should never attend another meeting, but the truth is that we say yes to many things we don't actually want to do. There are many meetings held that don't need to be held. There is a lot of code written that could be deleted.

How often do people ask you to do something and you just reply, "Sure thing." Three days later, you're overwhelmed by how much is on your to-do list. We become frustrated by our obligations even though we were the ones who said yes to them in the first place.

It's worth asking if things are necessary. Many of them are not, and a simple "no" will be more productive than whatever work the most efficient person can muster.

But if the benefits of saying no are so obvious, then why do we say yes so often?

We agree to many requests not because we want to do them, but because we don't want to be seen as rude, arrogant, or unhelpful. Often, you have to consider saying no to someone you will interact with again in the futureâ€”your co-worker, your spouse, your family and friends.
"""

print("\n" + "=" * 50)
print("Custom Text Summary:")
print("=" * 50)
custom_summary = summarize_text(custom_text)
print(custom_summary)

# Calculate ROUGE scores for evaluation
print("\n" + "=" * 50)
print("Calculating ROUGE scores on 5 test samples...")
print("=" * 50)

rouge = evaluate.load('rouge')

predictions = []
references = []

for i in range(5):
    text = ds['test'][i]['dialogue']
    reference = ds['test'][i]['summary']
    
    prediction = summarize_text(text)
    
    predictions.append(prediction)
    references.append(reference)
    
    print(f"\nSample {i+1}:")
    print(f"Reference: {reference}")
    print(f"Prediction: {prediction}")

# Compute ROUGE scores
results = rouge.compute(predictions=predictions, references=references)
print("\n" + "=" * 50)
print("ROUGE Evaluation Results:")
print("=" * 50)
for key, value in results.items():
    print(f"{key}: {value:.4f}")

print("\n" + "=" * 50)
print("Notebook execution completed successfully!")
print("=" * 50)
